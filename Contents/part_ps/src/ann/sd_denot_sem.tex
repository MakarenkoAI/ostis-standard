\begin{scnsubstruct}

\scnheader{Денотационная семантика Языка представления нейросетевого метода решения задач}
\scntext{примечание}{Денотационная семантика Языка представления нейросетевого метода решения задач в базах знаний описывается в рамках предметной области и соответствующей ей онтологии нейросетевого метода.}
\scntext{примечание}{Так же в \textit{Предметную область нейросетевых методов} добавлены понятия для описания метрик эффективности \textit{нейросетевых методов}. Данные метрики учитываются \textit{решателем задач} при принятии решения об использовании того или иного \textit{нейросетевого метода}.}

\scnheader{искусственная нейронная сеть}
\scnidtf{и.н.с.}
\scnidtf{множество искусственных нейронных сетей}
\scnidtf{нейронная сеть}
\scnidtf{нейросетевой метод}
\scntext{определение}{\textbf{\textit{искусственная нейронная сеть}} --- это совокупность нейронных элементов и связей между ними.}
\scntext{примечание}{Искусственная нейронная сеть состоит из \textbf{\textit{формальных нейронов}}, которые связаны между собой посредством \textbf{\textit{синаптических связей}}. Нейроны организованы в \textbf{\textit{слои}}. Каждый нейрон слоя принимает сигналы со входящих в него синаптических связей, обрабатывает их единым образом с помощью заданной ему или всему слою \textbf{\textit{функции активации}} и передает результат на выходящие из него синаптические связи.}
\begin{scnindent}
	\begin{scnrelfromset}{источник}
		\scnitem{\scncite{Golovko2017}}
	\end{scnrelfromset}
\end{scnindent}

\scnheader{архитектура и.н.с.}
\scntext{примечание}{\textit{Архитектурой и.н.с.} будем называть совокупность информации о структуре ее слоев, формальных нейронов, синаптических связей и функций активаций. То есть то, что можно обучить и использовать для решения задач.}
\scnrelfrom{пример}{\scnfileimage[30em]{Contents/part_ps/src/images/sd_ps/sd_ann/neural_network.png}}
\begin{scnindent}
	\scntext{примечание}{Пример архитектуры и.н.с.}
\end{scnindent}

\scnheader{искусственная нейронная сеть}
\scntext{примечание}{В соответствии с тем, какая у и.н.с. архитектура, можно выделить соответствующую иерархию классов и.н.с.}
\scnrelfrom{разбиение}{\scnkeyword{Типология и.н.с. по признаку направленности связей\scnsupergroupsign}}
\begin{scnindent}
	\begin{scneqtoset}
		\scnitem{искусственная нейронная сеть с прямыми связями}
		\begin{scnindent}
			\begin{scnsubdividing}
				\scnitem{персептрон}
				\begin{scnindent}
					\begin{scnsubdividing}
						\scnitem{персептрон Розенблатта}
						\scnitem{автоэнкодерная искусственная нейронная сеть}
					\end{scnsubdividing}
				\end{scnindent}
				\scnitem{машина опорных векторов}
				\scnitem{искусственная нейронная сеть радиально-базисных функций}
				\scnitem{сверточная искусственная нейронная сеть}
			\end{scnsubdividing}
		\end{scnindent}
		\scnitem{искусственная нейронная сеть с обратными связями}
		\begin{scnindent}
			\begin{scnsubdividing}
				\scnitem{нейронная сеть Хопфилда}
				\scnitem{нейронная сеть Хэмминга}
			\end{scnsubdividing}
		\end{scnindent}
		\scnitem{рекуррентная искусственная нейронная сеть}
		\begin{scnindent}
			\begin{scnsubdividing}
				\scnitem{искусственная нейронная сеть Джордана}
				\scnitem{искусственная нейронная сеть Элмана}
				\scnitem{мультирекуррентная нейронная сеть}
				\scnitem{LSTM-элемент}
				\scnitem{GRU-элемент}
			\end{scnsubdividing}
		\end{scnindent}
	\end{scneqtoset}
\end{scnindent}
\scnrelfrom{разбиение}{\scnkeyword{Типология и.н.с. по признаку полноты связей\scnsupergroupsign}}
\begin{scnindent}
	\begin{scneqtoset}
		\scnitem{полносвязная искусственная нейронная сеть}
		\scnitem{слабосвязная искусственная нейронная сеть}
	\end{scneqtoset}
\end{scnindent}

\scnheader{формальный нейрон}
\scnidtf{искусственный нейрон}
\scnidtf{нейрон}
\scnidtf{ф.н.}
\scnidtf{нейронный элемент}
\scnidtf{множество нейронов искусственных нейронных сетей}
\scnidtf{математическая модель реального биологического нейрона}
\scntext{примечание}{Отдельный формальный нейрон является искусственной нейронной сети с одним нейроном в единственном слое.}
\scnsubset{искусственная нейронная сеть}
\scntext{пояснение}{\textbf{\textit{формальный нейрон}} --- это основной элемент \textit{искусственной нейронной сети}, применяющий свою \textit{функцию активации} к сумме произведений входных сигналов на весовые коэффициенты:
	$$y = F\left(\sum_{i=1}^{n} w_ix_i - T\right) = F(WX - T)$$
	где $X = (x_1,x_2,...,x_n)^T$ --- вектор входного сигнала; $W - (w_1,w_2,...,w_n)$ --- вектор весовых коэффициентов; $T$ --- пороговое значение;
	\textit{F} --- функция активации.}
\begin{scnindent}
	\begin{scnrelfromset}{источник}
		\scnitem{\scncite{Golovko2017}}
	\end{scnrelfromset}
\end{scnindent}
\scnrelfrom{изображение}{\scnfileimage[20em]{Contents/part_ps/src/images/sd_ps/sd_ann/neuron.png}}
\begin{scnindent}
	\scntext{примечание}{Схема модели формального нейрона.}
\end{scnindent}
\scntext{примечание}{Формальные нейроны могут иметь полный набор связей с нейронами предшествующего слоя или неполный (разряженный) набор связей.}
\begin{scnsubdividing}
	\scnitem{полносвязный формальный нейрон}
	\begin{scnindent}
		\scnidtf{нейрон, у которого есть полный набор связей с нейронами предшествующего слоя}
		\scntext{пояснение}{Отдельный обрабатывающий элемент и.н.с., выполняющий функциональное преобразование взвешенной суммы элементов вектора входных значений с помощью функции активации.}
	\end{scnindent}
	\scnitem{сверточный формальный нейрон}
	\begin{scnindent}
		\scntext{пояснение}{Отдельный обрабатывающий элемент и.н.с., выполняющий функциональное преобразование результата операции свертки матрицы входных значений с помощью функции активации.}
		\scntext{примечание}{Сверточный формальный нейрон может быть представлен полносвязным формальным нейроном.}
		\scntext{примечание}{Сверточный формальный нейрон с соответствующим ему ядром свертки может быть представлен нейроном с неполным набором связей.}
	\end{scnindent}
	\scnitem{рекуррентный формальный нейрон}
	\begin{scnindent}
		\scntext{пояснение}{Формальный нейрон, имеющий обратную связь с самим собой или с другими нейронами и.н.с.}
	\end{scnindent}
\end{scnsubdividing}

\scnheader{синаптическая связь}
\scnidtf{синапс}
\scnsubset{ориентированная пара}
\scndefinition{\textbf{\textit{синаптическая связь}} --- ориентированная пара, первым компонентом которой является нейрон, из которого исходит сигнал, а вторым компонентом --- нейрон, который принимает этот сигнал.}

\scnheader{слой и.н.с.}
\scnidtf{слой}
\scnidtf{слой искусственной нейронной сети}
\scnidtf{множество слоев искусственных нейронных сетей}
\scntext{примечание}{Отдельный слой является искусственной нейронной сетью с одним слоем. Следует отметить принципиальную важность этого замечания. Один слой и.н.с. уже является нейронной сетью, поскольку над ним можно производить все основные операции, которые производятся над \scnqq{большой} и.н.с. (его можно обучить и использовать для решения определенной задачи).}
\scnsubset{искусственная нейронная сеть}
\scntext{пояснение}{\textbf{\textit{слой и.н.с}} --- это множество нейронных элементов, на которые в каждый такт времени параллельно поступает информация от других нейронных элементов сети.}
\begin{scnindent}
	\begin{scnrelfromset}{источник}
		\scnitem{\scncite{Golovko2017}}
	\end{scnrelfromset}
\end{scnindent}
\scntext{пояснение}{\textbf{\textit{слой и.н.с.}} --- это множество формальных нейронов, осуществляющих параллельную независимую обработку вектора или матрицы входных значений}
\scnrelfrom{разбиение}{\scnkeyword{Типология слоев и.н.с. по признаку операции, осуществляемой слоем\scnsupergroupsign}}
\begin{scnindent}
	\begin{scneqtoset}
	\scnitem{полносвязный слой и.н.с.}
	\begin{scnindent}
		\scnidtf{слой, в котором каждый нейрон имеет связь с каждым нейроном предшествующего слоя}
		\scnidtf{слой, в котором каждый нейрон является полносвязным}
	\end{scnindent}
	\scnitem{сверточный слой и.н.с.}
	\begin{scnindent}
		\scnidtf{слой, в котором каждый нейрон является сверточным}
	\end{scnindent}
	\scnitem{слой и.н.с. нелинейного преобразования}
	\begin{scnindent}
		\scnidtf{слой, осуществляющий нелинейное преобразование входных данных}
		\scntext{пояснение}{Как правило, выделяются в отдельные слои только в программных реализациях. Фактически рассматриваются как финальный этап расчета выходной активности любого нейрона --- применение функции активации.}
		\scntext{примечание}{не изменяет размерность входных данных}
	\end{scnindent}
	\scnitem{dropout слой и.н.с.}
	\begin{scnindent}
		\scnidtf{слой, реализующий технику регуляризации dropout}
		\scntext{примечание}{Данный тип слоя функционирует только во время обучения и.н.с.}
		\scntext{пояснение}{Поскольку полносвязные слои имеют большое количество настраиваемых параметров, они подвержены эффекту переобучения. Один из способов устранить такой негативный эффект --- выполнить частичный отсев результатов на выходе полносвязного слоя. На этапе обучения техника dropout позволяет отбросить выходную активность некоторых нейронов с определенной, заданной вероятностью. Выходная активность \scnqqi{отброшенных} нейронов полагается равной нулю.}
	\end{scnindent}
	\scnitem{pooling слой и.н.с.}
	\begin{scnindent}
		\scnidtf{подвыборочный слой}
		\scnidtf{объединяющий слой}
		\scnidtf{слой, осуществляющий уменьшение размерности входных данных}
	\end{scnindent}
	\scnitem{слой и.н.с. батч-нормализации}
	\end{scneqtoset}
	\begin{scnindent}
		\scntext{примечание}{Нужно отметить, что данный перечень неполный --- разновидности слоев и.н.с. появляются практически в каждой заслуживающей внимания публикации по нейросетевым алгоритмам и на текущий момент их существует достаточно много, однако, как правило, при построении более традиционных архитектур ограничиваются только приведенными вариантами слоев.}
	\end{scnindent}
\end{scnindent}
\scntext{примечание}{слои и.н.с. также могут быть классифицированы по исполняемой роли в рамках архитектуры (место в последовательности слоев и.н.с.).\\
	\\Так, например, слой, расположенный первым, называется распределяющим. Слои, расположенные далее, за исключением последнего, называются обрабатывающими. Наконец, последний слой носит название выходного слоя и.н.с.}


\scnheader{функция активации*}
\scnidtf{функция активации нейрона*}
\scniselement{неролевое отношение}
\scniselement{бинарное отношение}
\scntext{примечание}{функция активации* --- последний архитектурный компонент и.н.с.}
\scntext{пояснение}{\textbf{\textit{функция активации*}} --- неролевое отношение, связывающее формальный нейрон с функцией, результат применения которой к \textbf{\textit{взвешенной сумме нейрона}} определяет его \textbf{\textit{выходное значение}}.}
  \scnrelfrom{область определения}{\scnnonamednode}
  \begin{scnindent}
	  \begin{scnreltoset}{объединение}
		  \scnitem{формальный нейрон}
		  \scnitem{функция}
	  \end{scnreltoset}
  \end{scnindent}
\scnrelfrom{первый домен}{формальный нейрон}
\scnrelfrom{второй домен}{функция}
\begin{scnindent}
\begin{scnsubdividing}
	\scnitem{линейная функция}
		\begin{scnindent}
			\scntext{формула}{
				\begin{equation*}
					y = kS
				\end{equation*}}
				\begin{scnindent}
					\scntext{примечание}{\textit{k} --- коэффициент наклона прямой, \textit{S} --- в.с.}
				\end{scnindent}
		\end{scnindent}
	\scnitem{пороговая функция}
		\begin{scnindent}
			\scntext{формула}{
				\begin{equation*}
					y = sign(S) =
					\begin{cases}
						1, S > 0,\\
						0, S \leq 0
					\end{cases}
				\end{equation*}}
		\end{scnindent}
	\scnitem{сигмоидная функция}
		\begin{scnindent}
			\scntext{формула}{
				\begin{equation*}
					y = \frac{1}{1+e^{-cS}}
				\end{equation*}}
				\begin{scnindent}
					\scntext{примечание}{\textit{с} > 0 --- коэффициент, характеризующий ширину сигмоидной функции по оси абсцисс, \textit{S} --- в.с.}
				\end{scnindent}				
		\end{scnindent}
	\scnitem{функция гиперболического тангенса}
		\begin{scnindent}
			\scntext{формула}{
				\begin{equation*}
					y = \frac{e^{cS}-e^{-cS}}{e^{cs}+e^{-cS}}
				\end{equation*}}
				\begin{scnindent}
					\scntext{примечание}{\textit{с} > 0 --- коэффициент, характеризующий ширину сигмоидной функции по оси абсцисс, \textit{S} --- в.с.}
				\end{scnindent}
		\end{scnindent}
	\scnitem{функция softmax}
		\begin{scnindent}
			\scntext{формула}{
				\begin{equation*}
					y_j = softmax(S_j) = \frac{e^{S_j}}{\sum_{j} e^{S_j}}
				\end{equation*}}
				\begin{scnindent}
					\scntext{примечание}{$S_j$ --- в.с. \textit{j}-го выходного нейрона.}
				\end{scnindent}
		\end{scnindent}
	\scnitem{функция ReLU}
		\begin{scnindent}
			\scntext{формула}{
				\begin{equation*}
					y = F(S) =
					\begin{cases}
						S, S > 0,\\
						kS, S \leq 0
					\end{cases}
				\end{equation*}}
				\begin{scnindent}
					\scntext{примечание}{\textit{k} = 0 или принимает небольшое значение, например, 0.01 или 0.001.}
				\end{scnindent}
		\end{scnindent}
\end{scnsubdividing}
\end{scnindent}

\scnheader{параметр и.н.с.}
    \scnsubset{параметр}
    \begin{scnsubdividing}
        \scnitem{настраиваемый параметр и.н.с.}
        \begin{scnindent}
            \scnidtf{параметр и.н.с., значение которого изменяется в ходе обучения}
            \begin{scnsubdividing}
                \scnitem{весовой коэффициент синаптической связи}
                \scnitem{пороговое значение}
                \scnitem{ядро свертки}
                \begin{scnindent}
                    \scnidtf{квадратная матрица произвольного порядка, элементы которой изменяются в процессе обучения и.н.с.}
                    \scntext{примечание}{Если сверточный формальный нейрон представить в виде полносвязного формального нейрона, соответствующее ядро свертки преобразуется в вектор весовых коэффициентов.}
                \end{scnindent}
            \end{scnsubdividing}
        \end{scnindent}
        \scnitem{архитектурный параметр и.н.с.}
        \begin{scnindent}
            \scntext{примечание}{Параметр и.н.с., определяющий ее архитектуру.}
            \begin{scnsubdividing}
                \scnitem{количество слоев}
                \scnitem{количество нейронов}
                \scnitem{количество синапсов}
            \end{scnsubdividing}
        \end{scnindent}
    \end{scnsubdividing}

\scnheader{метрика оценки качества и.н.с.}
\scntext{примечание}{Метрики могут быть классифицированы по типу решаемой задачи.}
\scnrelfrom{разбиение}{Типология метрик по признаку решаемой задачи\scnsupergroupsign}
\begin{scnindent}
	\begin{scneqtoset}
		\scnitem{классификационные метрики}
		\begin{scnrelfromset}{декомпозиция}
			\scnitem{точность и.н.с.}
			\scnitem{полнота и.н.с.}
			\scnitem{F1-метрика}
		\end{scnrelfromset}
		\scnitem{регрессионные метрики}
		\begin{scnrelfromset}{декомпозиция}
			\scnitem{MAE}
			\scnitem{MAPE}
			\scnitem{RMSE}
		\end{scnrelfromset}
	\end{scneqtoset}
\end{scnindent}

\scnheader{точность и.н.с.}
\scnidtf{precision}
\scnidtf{доля верно идентифицированных положительных исходов в общем числе исходов, которые были идентифицированы как положительные}
\scntext{формула}{
	\begin{equation*}
		PRE = \frac{TP}{TP + FP}
	\end{equation*}}
	\begin{scnindent}
        \scntext{примечание}{\textit{TP} и \textit{FP} --- число истинно-положительных и ложно-положительных предсказаний нейронной сети соответственно}
    \end{scnindent}

\scnheader{полнота и.н.с.}
\scnidtf{recall}
\scnidtf{доля верно идентифицированных положительных исходов в общем числе положительных исходов}
\scntext{формула}{
	\begin{equation*}
		REC = \frac{TP}{TP + FN}
	\end{equation*}}
	\begin{scnindent}
        \scntext{примечание}{\textit{TP} и \textit{FN} --- число истинно-положительных и ложно-отрицательных предсказаний нейронной сети соответственно}
    \end{scnindent}

\scnheader{F1-метрика}
\scntext{формула}{
	\begin{equation*}
		F1 = 2 * \frac{PRE * REC}{PRE + REC}
	\end{equation*}}
	\begin{scnindent}
        \scntext{примечание}{\textit{PRE} и \textit{REC} --- точность и полнота и.и.с. соответственно}
    \end{scnindent}

\scnheader{MAE}
\scnidtf{mean absolute error}
\scntext{формула}{$\frac{1}{N} \sum_{i=1}^N |y_{etalon}^i - y_{predicted}^i|$}
\begin{scnindent}
	\scntext{примечание}{$y_{etalon}^i$ --- эталонное значение,\\ $y_{predicted}^i$ --- значение, полученное и.н.с.,\\ \textit{N} --- объем обучающей выборки}
\end{scnindent}

\scnheader{MAPE}
\scnidtf{mean absolute percentage error}
\scntext{формула}{$\frac{1}{N} \sum_{i=1}^N \frac{|y_{etalon}^i - y_{predicted}^i|}{y_{etalon}^i} * 100\%$}
\begin{scnindent}
	\scntext{примечание}{$y_{etalon}^i$ --- эталонное значение,\\ $y_{predicted}^i$ --- значение, полученное и.н.с.,\\ \textit{N} --- объем обучающей выборки}
\end{scnindent} 

\scnheader{RMSE}
\scnidtf{root mean squared error}
\scntext{формула}{$\sqrt{\frac{1}{N} \sum_{i=1}^N (y_{etalon}^i - y_{predicted}^i)^2}$}
\begin{scnindent}
	\scntext{примечание}{$y_{etalon}^i$ --- эталонное значение,\\ $y_{predicted}^i$ --- значение, полученное и.н.с.,\\ \textit{N} --- объем обучающей выборки}
\end{scnindent}

\scnheader{SCg-текст. Пример формализации архитектуры искусственной нейронной сети в базе знаний}
\scnrelfrom{изображение}{\scnfileimage[20em]{Contents/part_ps/src/images/sd_ps/sd_ann/neural_network_scg.png}}

\bigskip
\end{scnsubstruct}

\begin{scnrelfromvector}{заключение}
	\scnfileitem{С помощью выделенных понятий становится возможна формализация в \textit{базе знаний} архитектуры конкретных \textit{и.н.с.} В качестве примера, на рисунке \textit{SCg-текст. Пример формализации архитектуры искусственной нейронной сети в базе знаний} представлен пример формализации полносвязной двухслойной \textit{и.н.с.} с двумя нейронами на входном слое и одном нейроне на обрабатывающем слоев.}
	\scnfileitem{Следует отметить, что в практике авторов еще не было необходимости явно представлять и.н.с., как это показано на рисунке \textit{SCg-текст. Пример формализации архитектуры искусственной нейронной сети в базе знаний}. Чаще всего, представление и.н.с. сводилось к представлению ее операционной семантики в виде SCP-программы, как это будет показано далее.}
\end{scnrelfromvector}

\scnendcurrentsectioncomment
